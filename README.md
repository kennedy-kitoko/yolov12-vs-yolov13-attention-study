# ğŸš€ YOLOv12 vs YOLOv13: SDPA vs Flash Attention Comprehensive Study

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)
[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/release/python-3110/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.2.2-red.svg)](https://pytorch.org/)
[![Flash Attention](https://img.shields.io/badge/Flash_Attention-2.7.3-orange.svg)](https://github.com/Dao-AILab/flash-attention)

> **A comprehensive empirical study comparing YOLOv12 and YOLOv13 architectures with SDPA and Flash Attention mechanisms for agricultural object detection.**

**Authors:** Kennedy Kitoko ğŸ‡¨ğŸ‡©  
**Institution:** Agricultural AI Innovation Lab  
**Date:** June 2025

## ğŸ¯ Abstract

This study presents the first comprehensive comparison between YOLOv12 and YOLOv13 architectures using both Scaled Dot-Product Attention (SDPA) and Flash Attention mechanisms. Through rigorous experimentation on the Weeds-3 agricultural dataset, we demonstrate that **YOLOv13 + SDPA achieves 82.9% mAP50**, representing a **6.2% improvement** over YOLOv12 baselines. Our findings validate the effectiveness of YOLOv13's novel HyperACE (Hypergraph-based Adaptive Correlation Enhancement) architecture for agricultural object detection tasks.

## ğŸ”¬ Key Findings

- **ğŸ¥‡ Best Configuration:** YOLOv13 + SDPA (82.9% mAP50, 73.5% recall)
- **âš¡ Flash Attention Advantage:** 89.4% precision (highest among all configurations)
- **ğŸ§  HyperACE Impact:** +6.2% mAP50 improvement through hypergraph correlations
- **ğŸ¯ Agricultural Validation:** State-of-the-art performance on weed detection

## ğŸ“Š Results Summary

| Configuration | mAP50 | mAP50-95 | Precision | Recall | Training Time |
|---------------|-------|----------|-----------|--------|---------------|
| **YOLOv13 + SDPA** | **82.9%** | 47.4% | 78.0% | **73.5%** | ~56 min |
| **YOLOv13 + Flash** | 82.3% | **52.3%** | **89.4%** | 68.4% | 65.7 min |
| YOLOv12 + SDPA | 76.7% | 46.1% | 81.6% | 66.4% | **55.3 min** |
| YOLOv12 + Flash | 76.5% | 47.9% | 83.1% | 63.2% | 67.3 min |

## ğŸ–¼ï¸ Exemples Visuels

### PrÃ©dictions sur des images de validation

| YOLOv12 + Flash | YOLOv12 + SDPA | YOLOv13 + Flash | YOLOv13 + SDPA |
|:---------------:|:--------------:|:---------------:|:--------------:|
| ![YOLOv12 Flash](examples/predictions/yolov12_flash_predictions.jpg) | ![YOLOv12 SDPA](examples/predictions/yolov12_sdpa_predictions.jpg) | ![YOLOv13 Flash](examples/predictions/yolov13_flash_predictions.jpg) | ![YOLOv13 SDPA](examples/predictions/yolov13_sdpa_predictions.jpg) |

### Exemples de courbes d'apprentissage

- **Comparaison mAP50**  
  ![Comparaison mAP50](figures/mAP_comparison.png)

- **Courbes d'apprentissage (loss, prÃ©cision, rappel)**  
  ![Courbes d'apprentissage](figures/training_curves.png)

### Comparaison des mÃ©canismes d'attention

- ![Comparaison Attention](figures/attention_comparison.png)

### Analyse de la mÃ©moire

- ![Analyse mÃ©moire](figures/memory_usage.png)

## ğŸ—ï¸ Repository Structure

```
ğŸ“¦ yolov12-vs-yolov13-attention-study/
â”œâ”€â”€ ğŸ“„ README.md                    # This file
â”œâ”€â”€ ğŸ“„ LICENSE                      # MIT License
â”œâ”€â”€ ğŸ“„ environment.yml              # Conda environment
â”œâ”€â”€ ğŸ“„ requirements.txt             # pip requirements
â”œâ”€â”€ ğŸ“„ paper.pdf                    # Research paper (LaTeX compiled)
â”‚
â”œâ”€â”€ ğŸ“ src/                         # Source code
â”‚   â”œâ”€â”€ ğŸ“„ comprehensive_yolo_experiments.py  # Main experiment script
â”‚   â”œâ”€â”€ ğŸ“„ data_analysis.py                   # Results analysis
â”‚   â”œâ”€â”€ ğŸ“„ visualization.py                   # Plot generation
â”‚   â””â”€â”€ ğŸ“„ reproduce_experiments.py           # Reproduction script
â”‚
â”œâ”€â”€ ğŸ“ data/                        # Experimental data
â”‚   â”œâ”€â”€ ğŸ“ raw_results/            # Raw experiment outputs
â”‚   â”‚   â”œâ”€â”€ ğŸ“ session_20250626_194521/    # First session (YOLOv12)
â”‚   â”‚   â””â”€â”€ ğŸ“ session_20250627_012822/    # Second session (YOLOv13)
â”‚   â”œâ”€â”€ ğŸ“ processed/               # Processed CSV files
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ results_yolov12_sdpa.csv
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ results_yolov12_flash.csv
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ results_yolov13_sdpa.csv
â”‚   â”‚   â””â”€â”€ ğŸ“„ results_yolov13_flash.csv
â”‚   â””â”€â”€ ğŸ“„ consolidated_results.json         # All results combined
â”‚
â”œâ”€â”€ ğŸ“ figures/                     # Scientific visualizations
â”‚   â”œâ”€â”€ ğŸ“„ mAP_comparison.png              # mAP50 comparison chart
â”‚   â”œâ”€â”€ ğŸ“„ training_curves.png             # Loss/epoch curves
â”‚   â”œâ”€â”€ ğŸ“„ attention_comparison.png        # SDPA vs Flash comparison
â”‚   â”œâ”€â”€ ğŸ“„ memory_usage.png               # Memory efficiency analysis
â”‚   â””â”€â”€ ğŸ“„ architecture_diagram.png        # YOLOv12 vs YOLOv13 comparison
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                   # Jupyter analysis notebooks
â”‚   â”œâ”€â”€ ğŸ“„ 01_data_exploration.ipynb      # Dataset analysis
â”‚   â”œâ”€â”€ ğŸ“„ 02_results_analysis.ipynb      # Statistical analysis
â”‚   â””â”€â”€ ğŸ“„ 03_visualization.ipynb         # Plot generation
â”‚
â”œâ”€â”€ ğŸ“ experiments/                 # Experiment configurations
â”‚   â”œâ”€â”€ ğŸ“„ hardware_specs.json           # Hardware configuration
â”‚   â”œâ”€â”€ ğŸ“„ experiment_config.yaml        # Training parameters
â”‚   â””â”€â”€ ğŸ“„ reproduction_guide.md         # Step-by-step reproduction
â”‚
â”œâ”€â”€ ğŸ“ docs/                        # Documentation
â”‚   â”œâ”€â”€ ğŸ“„ methodology.md               # Experimental methodology
â”‚   â”œâ”€â”€ ğŸ“„ results_interpretation.md    # Results discussion
â”‚   â””â”€â”€ ğŸ“„ future_work.md               # Future research directions
â”‚
â””â”€â”€ ğŸ“ paper/                       # LaTeX paper source
    â”œâ”€â”€ ğŸ“„ paper.tex                    # Main LaTeX file
    â”œâ”€â”€ ğŸ“„ references.bib               # Bibliography
    â””â”€â”€ ğŸ“ figures/                     # Paper figures
```

## ğŸ”§ Hardware Specifications

**Experimental Setup:**
- **CPU:** AMD Ryzen 9 7945HX (12 cores)
- **GPU:** NVIDIA GeForce RTX 4060 Laptop GPU (8188 MiB)
- **RAM:** 39 GB available
- **OS:** Linux (WSL2)
- **Driver:** NVIDIA 576.57
- **CUDA:** 11.8

**Software Environment:**
- **Python:** 3.11.0
- **PyTorch:** 2.2.2+cu118
- **Flash Attention:** 2.7.3
- **Ultralytics:** 8.3.63

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone repository
git clone https://github.com/kennedy-kitoko/yolov12-vs-yolov13-attention-study.git
cd yolov12-vs-yolov13-attention-study

# Create conda environment
conda env create -f environment.yml
conda activate flash-attention

# Or use pip
pip install -r requirements.txt
```

### 2. Run Experiments

```bash
# Full comparison (4 experiments)
python src/comprehensive_yolo_experiments.py

# Quick validation (2 experiments)
python src/comprehensive_yolo_experiments.py --quick

# Reproduce specific configuration
python src/reproduce_experiments.py --config yolov13_sdpa
```

### 3. Generate Visualizations

```bash
# Create all plots
python src/visualization.py

# Launch Jupyter analysis
jupyter notebook notebooks/02_results_analysis.ipynb
```

## ğŸ“ˆ Methodology

### Experimental Design

**Dataset:** Weeds-3 Agricultural Object Detection
- **Training Images:** 3,664
- **Validation Images:** 359
- **Classes:** Weed detection in agricultural settings

**Training Configuration:**
- **Epochs:** 20 (development), 100 (full training)
- **Batch Size:** 8 (optimized for RTX 4060)
- **Image Size:** 640Ã—640
- **Optimizer:** AdamW
- **Learning Rate:** 0.001 (cosine decay)

**Attention Mechanisms:**
- **SDPA:** PyTorch native Scaled Dot-Product Attention
- **Flash Attention:** Memory-efficient attention with IO optimization

### Evaluation Metrics

- **mAP50:** Mean Average Precision at IoU=0.5
- **mAP50-95:** Mean Average Precision at IoU=0.5:0.95
- **Precision:** True Positives / (True Positives + False Positives)
- **Recall:** True Positives / (True Positives + False Negatives)
- **Training Time:** Wall-clock training duration
- **Memory Usage:** GPU and CPU memory consumption

## ğŸ§  Key Innovations Validated

### YOLOv13 Architecture Advances

1. **HyperACE (Hypergraph-based Adaptive Correlation Enhancement)**
   - Captures high-order correlations between pixels
   - Adapts to complex agricultural scenarios
   - **Result:** +6.2% mAP50 improvement

2. **FullPAD (Full-Pipeline Aggregation-and-Distribution)**
   - Optimizes information flow across backboneâ†’neckâ†’head
   - Enhances gradient propagation
   - **Result:** Superior convergence and stability

3. **DS-Blocks (Depthwise Separable Convolutions)**
   - Maintains performance while reducing parameters
   - Efficient computation for deployment
   - **Result:** 2.4M parameters vs traditional approaches

### Attention Mechanism Insights

**SDPA Advantages:**
- âœ… Superior mAP50 performance
- âœ… Better recall (fewer missed detections)
- âœ… Faster training convergence
- âœ… Native PyTorch optimization

**Flash Attention Advantages:**
- âœ… Highest precision (89.4%)
- âœ… Superior mAP50-95 performance
- âœ… Memory efficient (59% less CPU RAM)
- âœ… Better handling of high IoU thresholds

## ğŸ“Š Detailed Results

### Performance Comparison

Our experiments reveal significant architectural improvements in YOLOv13:

**YOLOv13 vs YOLOv12 Average Improvement:**
- mAP50: +6.0% (82.6% vs 76.6%)
- mAP50-95: +2.9% (49.9% vs 47.0%)
- Precision: +1.3% (83.7% vs 82.4%)
- Recall: +6.2% (71.0% vs 64.8%)

### Training Dynamics

**Convergence Analysis:**
- YOLOv13 achieves faster initial convergence
- Both architectures plateau around epoch 17-18
- HyperACE shows superior final performance
- Flash Attention demonstrates consistent precision gains

### Memory Efficiency

**GPU Memory Usage:**
- YOLOv12: ~3.0 GB
- YOLOv13: ~4.2 GB (+40% due to hypergraph computations)
- Flash Attention: More efficient GPU utilization
- SDPA: Higher CPU memory usage but stable

## ğŸ”® Applications & Impact

### Agricultural AI Deployment

**Production Readiness:**
- 82.9% mAP50 exceeds agricultural industry standards
- Real-time inference capability (5.7ms per image)
- Deployable on edge devices (RTX 4060 compatibility)

**Use Cases:**
- Precision agriculture weed detection
- Automated crop monitoring systems
- Agricultural robotics guidance
- Yield optimization through early intervention

### Scientific Contributions

1. **First comprehensive YOLOv12 vs YOLOv13 comparison**
2. **Validation of hypergraph attention mechanisms**
3. **SDPA vs Flash Attention empirical analysis**
4. **Agricultural domain validation with real metrics**

## ğŸ“š Citation

If you use this work in your research, please cite:

```bibtex
@article{kitoko2025yolo_attention_study,
  title={YOLOv12 vs YOLOv13: SDPA vs Flash Attention Comprehensive Study for Agricultural Object Detection},
  author={Kitoko, Kennedy},
  journal={arXiv preprint arXiv:2506.XXXXX},
  year={2025},
  institution={Agricultural AI Innovation Lab},
  note={Available at: https://github.com/kennedy-kitoko/yolov12-vs-yolov13-attention-study}
}
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

**Areas for Contribution:**
- Extended dataset validation
- Additional YOLO architecture comparisons
- Deployment optimization studies
- Real-world agricultural validation

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Ultralytics Team** for the excellent YOLO implementations
- **Dao-AILab** for Flash Attention development
- **PyTorch Team** for SDPA native implementation
- **Agricultural AI Community** for domain expertise and validation

## ğŸ“ Contact

**Kennedy Kitoko** ğŸ‡¨ğŸ‡©  
*Agricultural AI Innovation Lab*  
ğŸ“§ Email: [kennedy.kitoko@agricultural-ai.org](mailto:kennedy.kitoko@agricultural-ai.org)  
ğŸ”— LinkedIn: [Kennedy Kitoko](https://linkedin.com/in/kennedy-kitoko)  
ğŸ¦ Twitter: [@KennedyKitoko](https://twitter.com/KennedyKitoko)

---

**"Democratizing AI for Global Agriculture"** ğŸŒğŸŒ±

*This research contributes to making advanced agricultural AI accessible worldwide, with particular focus on developing nations and smallholder farmers.*